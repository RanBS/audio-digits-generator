# audio-digits-generator

Generating audio files of spoken digits, using a conditional generative architectures (cVAE, cGAN) and evaluating the results with Inception Score.

GAN part is based on [this repository](https://github.com/gcucurull/cond-wgan-gp).

![spectograms](https://github.com/RanBS/audio-digits-generator/blob/main/assets/spectograms.gif)

- [audio-digits-generator](#audio-digits-generator)
  * [Background](#background)
  * [Main Prerequisites](#Main-Prerequisites)
  * [Files in the repository](#files-in-the-repository)
  * [How-to-use](#How-to-use)
  * [References](#references)

## Background
The purpose of the project is to create a generative model, that can generate audio files. More specifically, we chose to focus on speech audio files of digits.
Training a model to generate audio files based on their time series can be challenging, therefore we decided to use the STFT representation of the audio signal.
Generally, the STFT of a signal is complex and for that reason we will represent it as a 2 channel image, where the first one is the amplitude and the second one is the phase.
We examined two main generative architectures, conditional-VAE and conditional-wGAN-gp. For each architecture, we experimented with the different methods described below:
- VAE:
  * Generating a spectogram amplitude image only, conditioned on the label of a digit.
- GAN:
  * Experiment 1 - Generating a spectogram amplitude image only, conditioned on the label of a digit.
  * Experiment 2 - Generating a 2 channel image, of the spectogram's amplitude and phase, conditioned on the label of a digit.
  * Experiment 3 - Generating a spectogram amplitude image only, conditioned on both the label of a digit and a phase image compitable with the lable.
  * Experiment 4 - Same idea as Experiment 3, but with a regularization (explained in the PDF).
  
In addition to the generative models, we trained a digit-classifier based on the spectogram amplitude, for performance mesures perposes.


## Main Prerequisites
|Library         | Version |
|----------------------|----|
|`numpy`|  `1.19.5`|
|`torch`|  `1.6.0`|
|`librosa`|  `0.8.0`|
|`tqdm`|  `4.53.0`|
|`colorama`|  `0.4.4`|

## Files in the repository

|File name         | Purpsoe |
|----------------------|------|
|`inception_scores_metrics.py`| Evaluating a generative model performance with Frechet Inception Score and 'Diversity Score'|
|`pre_processing.py`| Converting .wav files to .npy for fitting the networks|
|`weights.txt`| Link to download weights|
|`metrics results for the exps.txt`| The performance of our trained generative models|

For each experiment, the following files are provided:

|File name         | Purpsoe |
|----------------------|------|
|`dataset.py`| Dataset class, that fits the pytorch conventions|
|`eval.py`| Loads a trained model and generates .wav files|
|`models.py`| The model (Generator & Discriminator / VAE)|
|`train.py`| Train the model|
|`Samples Directory`| Examples for samples generated by the trained model|
|`final_results_example.png`| 10 spectogram amplitudes generated from each label (each column is a specific label)|
|`train_log.txt`| The training progress|


## How-to-use



## References
* [PyTorch Agent Net: reinforcement learning toolkit for pytorch](https://github.com/Shmuma/ptan) by [Max Lapan](https://github.com/Shmuma)
* Nir Levine, Tom Zahavy, Daniel J. Mankowitz, Aviv Tamar, Shie Mannor [Shallow Updates for Deep Reinforcement Learning](https://arxiv.org/abs/1705.07461), NIPS 2017



